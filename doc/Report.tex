\documentclass[11pt]{article}

\begin{document}

\title{Super Optimisation of Programs}
\author{Thomas Rooney, Terence Tse : Group g1112069}
\maketitle

\section{Introduction and Background}
\indent When we were asked to perform an unspecified extension of the assembler/emulator, one of Tom's immediate thoughts was program optimisation. Initially disregarding this as far too complicated, Tom happened upon a bit of creativity. One of the opensource projects that he follows is known as the openCog project - short for 'Open Cognition' and is a project with an ultimate aim to build an AGI(Artificial General Intelligence). In the process of doing so over a long period of time, they have written and integrated some very effective pieces of A.I. code. 
One of these is known as MOSES. This stands for Meta-Optimising Semantic Evolutionary Search, and it can generate simple algebraic programs based around either a scoring function or test data which the programs will have produce. It was originally written by Moshe Looks (http://metacog.org/doc.html), for his PHD thesis: "Competent Program Evolution". Whilst following some of the current work going on for the development of the program, Tom recognised that the algebraic output it produced was very similar to assembler code, and it would perhaps only be a simple map between IMPS opcodes and MOSES's output. 
\\
\indent The project name Tom chose was superoptimisation. Unlike standard optimisation, when a superoptimiser is run for an infinite length of time, superoptimisation's result is, in theory, the fastest piece of code for a general problem.
\\
\indent To explore this idea, Tom sent off an email to the google groups page where the current discussion on the openCog project goes starts as below and still goes on:
\\
\\
Use of MOSES to optimize Assembly Code
\\
Hi All,
\\
I'm a student at Imperial College London, and I've been following
openCog for a while now, finding it an absolutely fascinating trove of A.I. techniques towards solving the AGI problem. One of the projects that particularly interests me is MOSES, and an idea which struck me recently is its use in optimization, specifically assembler code. 
\\
I am writing an assembler for a reduced instruction set as part of my university course, and have been looking at extensions that could be made upon it. I've attempted to go through the Moshe Looks's founding thesis "Competent Program Evolution", and I have found it to be beyond my current understanding, and as such cannot really make an accurate guess on how feasible the idea is. I am currently thinking of simply hooking 'expect' into its compiled binaries and mapping the assembler instruction tree I have into the format required by MOSES, to perform a proof of concept of the idea, as opposed to trying to work with atomspace and integrating MOSES source directly.
\\
It would be great if I could get some informed opinions on the feasibility of the idea, from those currently working with it.
\\
Kind Regards,
\\
Thomas Rooney
\\
\\
Ben Goertzel ben@goertzel.org via googlegroups.com 
\\This does seem possible, but would be more work than you're thinking...
\\The Atomspace would not need to be involved, but you'd need to create
primitive MOSES operations corresponding to the assembly language
operators, and then create Reduct rules embodying the algebraic
properties of these operators...
\\Then Reduct could put assembler programs in hierarchical normal form,
and you could use MOSES to search for programs equivalent to, but
faster than, the program you started with...
\\-- Ben
\\
\\
Linas Vepstas linasvepstas@gmail.com
\\The current technique that is gaining academic interest is called "superoptimization". This is done by building a database of millions of short instruction sequences; whenever the compiler emits one, you look it up, and see if there is an equivalent but shorter one. The database is created by simply enumerating all possible instruction sequences of 2,3,4,5 insns long. The hard part is deciding when two different insn sequences are identical. Its not enough to find some sequence that seems to be equivalent, one needs to actually *prove* that they're equivalent on all $2^64 times 2^64 = 2^128$ possible inputs; this is too large to test, but can be proven. One can decide this using "answer set programming" (which is what I would recommend), or by using "satsifiability modulo theories" (which some people are interested in). Its an interesting approach: you let machine learning discover equivalent sequences, rather than having humans work them out. Unfortunately, most of this work is at the PhD level; there are several recent theses which can be googled for this. Three's some interest in doing this for either gcc or clang.
\\Actually, its way cooler than that. When you have a prover, as above, then you can prove equivalence of snippets of C code. Which means you can do optimization on source, not on assembly. I actually prototyped this for a job, it worked :-)
\\I found that learning ASP gave me an entirely new perspective on PLN and similar ideas. ASP is very tricky at first, kind of hard to figure out, but once you get the hang of it, its actually fairly easy. It just forces you to think in a very different way. Oh, and you can't just re about it; you actually need to actually do something with it, to learn it.
\\
\\
Murilo Saraiva de Queiroz muriloq@gmail.com
\\Ricardo Bittencourt (who worked with us here in Brazil for a brief period and now it's at Google) did exactly this, but using as a target an 8-bit microprocessor (the Zilog Z80, quite similar to Intel 8085). Since the input space is much smaller (the number of processor registers and their widths is very small) it a lot more feasible... He used it to reduce the size of code submitted to 4-kilobyte game competitions. 
\\Here's the page (sorry, Portuguese only):
\\ http://www.ricbit.com/mundobizarro/superopt.php 
\\muriloq
\\Murilo Saraiva de Queiroz, MSc
\\Hardware Engineer at NVIDIA
\\
\\
Linas Vepstas linasvepstas@gmail.com
\\Not to give away secrets to nvidia, but: it's actually now feasible to do with 32 and 64 bit processors; the trick is to search for inputs where insn sequences are unequal; and modern-day ASP and SMT solvers are quite capable of this, being based on the modern SAT solver techniques.
\\
\\\hrulefill



\indent MOSES/Reduct was rather deeply integrated into the current opencog project, and Tom deemed it not worth the effort to try and extract the relevant details from the project, which doesn't fully compile under Cygwin anyway (which Tom is using). The options were to either use 'expect' and the provided Win32 Binaries to try and get something simple up and running, or try a more naive Levin Search implementation instead of the A.I. based solution. 
\\
\indent Research referenced in the email conversation on program optimisation for the Z80 processor showed that a naive implementation was possible, as it has been successfully used to reduce code used in a 'under-4k' game competetion. Using this as inspiration, a wrapper around the emulation program was written, that operates a naive genetic algorithm to try and optimise a compiled program by randomly changing and removing code from the 'scoring function' defined as the original, working code.





\section{Project description}
\indent The intention of this project extension is the superoptimisation of already compiled code. Through discussion we hoped that such a program able to carry out superoptimisation would be able to, if given an empty file and a desired goal output, be able to create fully functioning assembly code to produce this output with the limited instruction set given to us. After further discussion, Tom decided he would continue the project after its due date to see if he can build a decent superoptimisation solution to work with Intel Assembler (computing within a reasonable time frame of course). 
\\
\indent The project takes a file of compiled code for input. It then runs over the code and calculates a score. It then enters a genetic algorithm, which searches for the original score as a target (time and output) whilst executing a random mutation of the best program found. Once an improvement is found, this programs is now the new target, and the new program is the one mutated. 
\\
\\\indent This is a very naive implementation, though plans were drawn for several much faster ways of doing this. The favoured one was to find a 'Normal Form' upon which assembly programs could be elegantly placed into. Once this form was found, it would be possible to prove equivilence of different snippets of code very quickly. These would be entered into a (very large) database, meaning large amounts of code could be quickly optimised in parallel with the search process.
\\
\\
\indent Unfortately, the main problem found was discovering this 'normal form' by which assembler could be placed into. The simpler solution of running code in a sandbox whilst checking states before and after was much easier to implement.
\\
\indent Towards the end of the project, we did manage to get successful results with our program - even if the program to be optimised was fudged somewhat. The program simp\_opt2.s was produced, which has a useless loop, iterating to 1000. The hope was we could easily find a program which has NOP'd this loop or modified it in someway to speed it up - and success was found. The first solution to this that the program found was to replace the '+ 1' part of the loop with an arbitrarily large number '~16000'. Subsequently evolving the program later found that it could simply JMP over the loop (to the section which prints 'Hey'). 

\section{Design and Implementation}
\indent Though the initial “dream” extension of the superoptimiser was created; for now, we take a much simpler approach to the problem. Given the limited instruction set, we intend now to simply take in a sample assembly program, in this case it is one that simply computes the max of two given integers. From here, we decided that it would be best to simply go over the code and then randomly select lines to replace. We understood that this takes both a considerable amount of time and also is basically brute forcing optimisation which may take many, many tries. We did discuss other solutions, one would be that we could use the algorithm to store known simplifications of code. This would require use of a database, however, as we only have so many registers to utilise in this project. 
\\
\indent The program that we produced works with the arguments:
\\ ./superoptimiser <input file> <-s|-v|-h>
\\ This input file is 
\\
\indent However, before full termination, the program checks the new assembly code 100 times to gather a suitable amount of data to produce a trustworthy average of time to execute. The next step then follows on whether the average is better or worse than the original assembly code.
Some other examples of use could include:

\\
\indent There were also many other situations that we had thought of having a go at, but none really managing to come to fruition. For example, we did consider the fact that replacing  a line with several new lines could actually result in an improvement in time in some situations. There was also the fact that a change in a program such as our sample “Max” could simply be replaced with one out opcode and the register which conveniently holds the same answer as the original assembly program's output. These are all flaws within the current superoptimiser that we have. However, upon consultation amongst us both we realised that if we were even able to implement these cases, it would take an extremely long time to compute.
\\
\indent We also quickly realised that our original intention of using the newly generated optimised code (had one been found), to use as the benchmark for the next set of comparisons would also make running the program an extremely time costly task. As a result, we just decide to output the first new improvement we encounter when running the superoptimiser. 
\\
MORE OF THE PROBLEMS
\indent We also experienced Segmentation faults, these were the major problems as the code was runing for very long lenghts of time. Originally, we regularly malloc'd and freed data from the heap. Doing this over long lengths of time, i.e 24 hours, it was found that various segmentation faults were produced that did not occur in the original emulation code. Tom found out that the best way to fix these was to backtrace while running the cod in the debugger gdb. After doing so, he managed to locate the problems cauusing the segmentation faults and manully fixed them.

\section{Testing the Super Optimiser}
\indent We decided to test the superoptimiser periodically and modularly. 
\\After doing that, we used a sample assembly code file which computed the max of two supplied numbers. Using this as the input, the program ran against it and made various changes to it, though never overwriting the original file. Instead, we output the new code to stdout so that the user can view the newly optimised code and compare it against the original as well as check for possible errors.  
\indent Another test that we ran against it was to pass it another program, this was  program that contained a useless loop within it and we hoped that the program would be able to remove the loop and replace it. Having the loop within the program meant it mustered up an eecution time of aroun 300000 computer ticks. After passing it through the superoptimiser, we managed to get an improvement. This improvement ran up an execution time of around 1000 ticks, an extreme improvement. 
\\
\indent Upon inspecting the output code from the superoptimiser we found that there code had indeed changed. However, it was doing a varety of strange things such as implement a lot of addi instructions and allocating strange numbers into memory. None-the-less, the superoptimiser did its job, it optimised  program and made it faster than it was before through random removal and replacement of lines of assembly code. on that basis, we decided the count this as successful.

\section{Reflection upon the project}
\underline{Thomas' reflections}
\\
\indent This project was the most interesting and fun for me of the year - it allowed me to use C, one of my favourite languages due to the ease of understanding program flow, and the problem set was very open ended. The extension was particularly open-ended, and I initially pushed for merging open source AI algorithm, of which researching and attempting to merge was an interesting challenge, even though I failed. The final code is only just barely functional, and after crunching some numbers it would take several years to complete on any, even minor program. However, the implentation of program search which is shown is something I am proud of and will hopefully use as a stepping stone in future projects.
\\
\indent The group hygiene, in my opinion, worked well. We communicated regularly through various medium - Facebook/Skype/SMS, and met in person for several days where we worked continuously on the project - the most productive days. Terance was less confident in his C coding, but he picked it up relatively quickly, especially comparing it to my first fumbling attempts with the language last year. While I contributed the bulk of the code, and directed the project, Terence held his own in the debugging sessions, and I valued his input in curbing my ambitious tendencies towards the end of the project. 
\\

\noindent\underline{Terence's reflections}
\\ 
\indent This project posed many challenges to me; struggling to pick up C, it was a mixed blessing to be matched with my group mate, Tom, who has a hefty amount of experience with the language. Unfortunately due to his capabilities, Tom steamed ahead with the project and ended up producing  a large amount of the code while I had offered smaller additions in comparison. 
It was geat working with Tom but I did feel there were problems with group dynamics and synergy. I believe that my proficiency with the C language might have hindered Tom and the project as he was much more proficient and able to code from the get go. Early on, we had coding sessions where we met up and worked on the project but they mainly consisted of me asking questions and pointing out small errors. It may have been more beneficial to the two of us if we had been grouped with people of the same capabilities with the C language, though being matched with Tom definitely helped me improve my programming prowess. 
\\
\indent Dividing the work was difficult and Tom had a strong drive to press on with the project while I was preoccupied with learning the language itself! We had several meet ups for the coding of both the assembler and the emulator where we debugged and each coded and corrected one another. Eventually, we broke down the project into the 4 sections and had us assigned to individual pieces of the project although we did help each other with their respective sections.
\\
\indent In terms of communications within the group, since I hardly knew Tom at the beginning of the project, their was a period of only small bouts of conversation going on between us as we got to know each other better. However, as the project progressed, we eased up were able to ask/answer questions, point out things in code and converse about the project. Granted, if I had been matched with a person I already did know well, our synergy would probably have been better but this just serves as a learning experience for when I enter the world of work where this happens frequently.
\\
\indent The frequency of our meetings was moderate, I feel that I should have pushed to meet up in labs more often to go through the code more. However, our personal obligations and distance from the college did hamper the ability to do such things. this unfortunately lead to a lot of separate and remote working between us. As a result, we did not meet as often as I would have liked.
\\
\indent In my opinion, our weakest point was communication adn when it came to choosing what to do for this extension! I had not really given too much thought into it before Tom had chosen a project to start which interested him. Trusting Tom, we went over the basics of his idea and it impressed me to say the least, although it originally seemed out of my scope requirement. However, due to Tom's intention to progress further with it after the project and the interest he provoked as he explained to me the project, we proceeded. 
The one thing if anything that I wish to improve about my group working abilities is to make sure that all members are satisfied with the way the project is progressing and what decisions are being made. In this project, by the time I had found out about Tom's decision, he had already written several hundred lines of code and done considerable research (which I then had to catch up on!).

\indent This project has benefited me greatly and working with Tom has pushed me and made me pick up C faster than I would of. Working with someone so far from my programming capability has improved me as a programmer and I have picked up several useful tools and tips. It also helped me better understand "git", the fact that texmacs is an absolute pain to use and the stress of working within a group. I'd like to take the lessons I've learnt and apply them to the projects I shall embark on in the near future as it is clear I have a lot to improve.

\end{document}
